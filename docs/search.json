[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Entries",
    "section": "",
    "text": "Presidential Sentiment Analysis Project\n\n\n\n\n\nHow Do Different Sources of Media Impact an Opinion\n\n\n\n\n\n\nJan 5, 2025\n\n\nVanessa Salgado\n\n\n\n\n\n\n  \n\n\n\n\nMaster’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change\n\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nVanessa Salgado\n\n\n\n\n\n\n  \n\n\n\n\nCarbon Credits Data Visualization\n\n\n\n\n\nAnalyzing the Distribution of Carbon Credits between Countries\n\n\n\n\n\n\nMar 11, 2024\n\n\nVanessa Salgado\n\n\n\n\n\n\n  \n\n\n\n\nWater Stressors for Low Income Food Deficit Countries\n\n\n\n\n\n\n\nMEDS\n\n\nR\n\n\nStatistics\n\n\n\n\nExamining Water Use and Water Stress. Comparing Industrial, Irrigation, and Municipial Water Withdrawals\n\n\n\n\n\n\nDec 15, 2023\n\n\nVanessa Salgado\n\n\n\n\n\n\n  \n\n\n\n\n2021 Texas Blackout\n\n\n\n\n\nAnalyzing affected areas & socioeconomic factors that influenced recovery\n\n\n\n\n\n\nDec 15, 2023\n\n\nVanessa Salgado\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vanessa Salgado",
    "section": "",
    "text": "Hi! My name is Vanessa Salgado.\n\n\n\nUniversity of California, Santa Barbara | Santa Barbara, CA MEDS at Bren School of Environmental Science & Management"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Vanessa Salgado",
    "section": "",
    "text": "Hi! My name is Vanessa Salgado."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Vanessa Salgado",
    "section": "",
    "text": "University of California, Santa Barbara | Santa Barbara, CA MEDS at Bren School of Environmental Science & Management"
  },
  {
    "objectID": "posts/2023-12-15-texas/index.html#question-how-did-socioeconomic-factors-influence-community-recovery-from-2021-power-outages-in-houston-texas",
    "href": "posts/2023-12-15-texas/index.html#question-how-did-socioeconomic-factors-influence-community-recovery-from-2021-power-outages-in-houston-texas",
    "title": "2021 Texas Blackout",
    "section": "Question: How did socioeconomic factors influence community recovery from 2021 power outages in Houston, Texas?",
    "text": "Question: How did socioeconomic factors influence community recovery from 2021 power outages in Houston, Texas?\nrepo link | https://github.com/Vanessa-Salgado/houston-blackout-lights-analysis\n\nBackground\nIn February 2021, severe winter storms in the United States caused a major power outage in the state of Texas. The loss of power resulted in over 4.5 million homes and businesses left without power, and several deaths. This analysis uses remotely sensed night light data to assess the impact and distribution of these blackouts. Data from the U.S. Census Bureau will be added to investigate if socioeconomic factors affect the recovery of power within the community.\n\n\nProject objectives and methods\nIn this analysis, I will be: - estimating the number of homes in Houston that lost power as a result of the first two storms\n- investigating if socioeconomic factors are predictors of communities recovery from a power outage\nThis analysis will rely on remotely-sensed night lights data acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) aboard the Suomi satellite. Specifically, we will use the VNP46A1 to identify variations in night lights before and after the storms as a method to identify areas that lost electrical power."
  },
  {
    "objectID": "posts/2023-12-15-texas/index.html#data",
    "href": "posts/2023-12-15-texas/index.html#data",
    "title": "2021 Texas Blackout",
    "section": "Data",
    "text": "Data\n\nNight lights\nWe use NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\nRoads gis_osm_roads_free_1.gpkg\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\n\nHouses gis_osm_buildings_a_free_1.gpkg\nWe also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\nSocioeconomicACS_2019_5YR_TRACT_48.gdb\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file. We can use st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata. The geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "posts/2023-12-15-texas/index.html#outline-and-plan-of-analysis",
    "href": "posts/2023-12-15-texas/index.html#outline-and-plan-of-analysis",
    "title": "2021 Texas Blackout",
    "section": "Outline and Plan of Analysis",
    "text": "Outline and Plan of Analysis\nBelow is an outline of the steps taken to achieve the objectives.\n1) Find locations of blackouts\n2) Find homes impacted by blackouts\n3) Investigate Socioeconomic Factors\n4) Leverage the results for a summary and discussion"
  },
  {
    "objectID": "posts/2023-12-15-texas/index.html#importing-necessary-libraries-and-functions",
    "href": "posts/2023-12-15-texas/index.html#importing-necessary-libraries-and-functions",
    "title": "2021 Texas Blackout",
    "section": "Importing necessary libraries and functions",
    "text": "Importing necessary libraries and functions\n\n\nCode\nlibrary(sf)\nlibrary(stars)\nlibrary(tmap)\nlibrary(raster)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(here)"
  },
  {
    "objectID": "posts/2023-12-15-texas/index.html#find-locations-of-blackouts",
    "href": "posts/2023-12-15-texas/index.html#find-locations-of-blackouts",
    "title": "2021 Texas Blackout",
    "section": "Find locations of blackouts",
    "text": "Find locations of blackouts\nFor improved computational efficiency and easier inter-operability with sf, we use the stars package for raster handling.\n\nCombining the data\nWe first read in night lights tiles, then combine tiles into a single stars object for each date (2021-02-07 and 2021-02-16) using st_mosaic.\nThe following files had been subsetted & stored in the VNP46A1 folder.\n\nVNP46A1.A2021038.h08v05.001.2021039064328.tif: tile h08v05, collected on 2021-02-07\nVNP46A1.A2021038.h08v06.001.2021039064329.tif: tile h08v06, collected on 2021-02-07\nVNP46A1.A2021047.h08v05.001.2021048091106.tif: tile h08v05, collected on 2021-02-16\nVNP46A1.A2021047.h08v06.001.2021048091105.tif: tile h08v06, collected on 2021-02-16\n\n\n\nCreating a blackout mask\nWe then find the change in night lights intensity (presumably) caused by the storm, and reclassify the difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout. Then we assign NA to all locations that experienced a drop of less than 200 nW cm-2sr.-1\n\n\n\nCode\n#find the change in night lights intensity (presumably) caused by the storm by calculating the light difference between the two dates \ndifference &lt;- combined_02_07 - combined_02_16\n\n\n#check out difference in night lights intensity \nplot(difference)\n\n\ndownsample set to 6\n\n\n\n\n\nAssigning all the houses that experienced a drop of 200 and more to a blackout mask.\n\n\nCode\nmask_blackout &lt;- difference &gt; 200 \nmask_blackout[mask_blackout == FALSE]  &lt;- NA\n\n\n\n\nVectorize the mask\nWe use st_as_sf() to vectorize the blackout mask and fix any invalid geometries using st_make_valid.\n\n\nCode\nblackout_mask_vector &lt;- st_as_sf(mask_blackout) %&gt;% st_make_valid() ##can plot() to check out what the blackout mask looks like \n\n\n\n\nCropping the vectorized map to our region of interest\n\nWe first define the Houston metropolitan area with the following coordinates : (-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29) and then turn these coordinates into a polygon using st_polygon.\nThen we convert the polygon into a simple feature collection using st_sfc() and assign a CRS, making sure that the polygon is in the same CRS and crop (spatially subset) the blackout mask to our region of interest. Lastly we re-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area).\n\n\n\n\nCode\n#crop the vectorized map to our region of interest using polygon of Houston's coordinates \nhouston_polygon &lt;- st_polygon(list(rbind(c(-96.5,29), c(-96.5,30.5), c(-94.5, 30.5), c(-94.5,29), c(-96.5,29))))\n\n\n#convert the polygon into a simple feature collection using st_sfc() and assign the CRS 4326, which is the same as the night lights data\nhouston_border_sf &lt;- st_sfc(houston_polygon, crs = st_crs(mask_blackout))\n\n\n#inspect the polygon crs to make sure the crs is 4326, nightlights dataset\n#st_crs(houston_border_sf) == st_crs(mask_blackout)\n\n\nSpatially subsetting blackout mask to Houston region, then reprojecting to the correct CRS.\n\n\nExcluding highways from blackout mask\nThe roads geopackage includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query. First, we load just highway data from geopackage using st_read andreproject data to EPSG:3083, identifying areas within 200m of all highways using st_buffer. st_buffer produces undissolved buffers, so we use st_union to dissolve them. Then we find areas that experienced blackouts that are further than 200m from a highway.\nRead in highway data from roads using SQL query :\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nhighways &lt;- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = query)\n\n\nCode\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nhighways &lt;- st_read(here(\"data/gis_osm_roads_free_1.gpkg\"), query = query)\n\n\nThen reproject and subset using st_difference()\n\n\nCode\n# reproject data to EPSG:3083 \nhighways_3038 &lt;- st_transform(highways, crs=\"EPSG:3083\")\n\n#verify it's the right crs\n#st_crs(highways_3038)\n\n# identify areas within 200m of all highways using st_buffer. Use st_union to dissolve overlapping polygons. \na_near_hwys &lt;- st_buffer(highways_3038, dist = 200) %&gt;% st_union()\n\n# find areas that experienced blackouts that are further than 200m from a highway by spacially subsetting houston blackout areas to ones near highways, and taking st_difference of that. \nblackouts_far_hwy_houston &lt;- final_houston_blackout[a_near_hwys, , op = st_difference] \n\n\n#plot to see what it looks like \n#plot(blackouts_far_hwy_houston)"
  },
  {
    "objectID": "posts/2023-12-15-texas/index.html#finding-homes-impacted-by-blackouts",
    "href": "posts/2023-12-15-texas/index.html#finding-homes-impacted-by-blackouts",
    "title": "2021 Texas Blackout",
    "section": "Finding homes impacted by blackouts",
    "text": "Finding homes impacted by blackouts\n\nLoad buildings data\nWe load buildings dataset using st_read and the following SQL query to select only residential buildings. We first reproject data to EPSG:3083.\nThe SQL query we will use is:\nSELECT *  FROM gis_osm_buildings_a_free_1\nWHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\n\n\n\nCode\n#load buildings dataset using st_read and the following SQL query to select only residential buildings\n\nquery_buildings &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\nbuildings &lt;- st_read(here(\"data\",\"gis_osm_buildings_a_free_1.gpkg\"), query = query_buildings)\n\n\n\n\nfinding homes in blackout areas\nWe filter to homes within blackout areas, then we count number of impacted homes. First, checking for CRS compatibility. Since they aren’t, we need to transform and then subset.\n\n\nCode\n##check CRS's are the same. No, they're not!\n#st_crs(buildings) == st_crs(blackouts_far_hwy_houston)\n\n##transform buildings crs \nbuildings &lt;- st_transform(buildings, crs = st_crs(blackouts_far_hwy_houston))\n\n# filtering the houses data with the blackout mask\nbuildings_blackout &lt;- buildings[blackouts_far_hwy_houston, drop = FALSE]\n\n## View the data frame for inspection \n#View(buildings_blackout)\n\n#count number of impacted homes\nprint(paste0(nrow(buildings_blackout), \" Houston homes experienced power outage due to the Texas storms in Feburary, 2021\"))\n\n\n[1] \"164867 Houston homes experienced power outage due to the Texas storms in Feburary, 2021\""
  },
  {
    "objectID": "posts/2023-12-15-texas/index.html#investigate-socioeconomic-factors",
    "href": "posts/2023-12-15-texas/index.html#investigate-socioeconomic-factors",
    "title": "2021 Texas Blackout",
    "section": "Investigate socioeconomic factors",
    "text": "Investigate socioeconomic factors\n\nload ACS data\nTo set up for loading, we use st_read() to load the geodatabase layers and geometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer. The income data is stored in the X19_INCOME layer, so we select the median income field B19013e1 andreproject data to EPSG:3083.\n\n\nDetermine which census tracts experienced blackouts\nWe join the income data to the census tract geometries by joining by geometry ID and spatially join census tract data with buildings determined to be impacted by blackouts, then find which census tracts had blackouts.\n\n\n\nCode\n#determine which census tracts experienced blackouts \n#Join the income data to the census tract geometries. Join by geometry ID\nmedian_income_df &lt;- median_income %&gt;% rename(GEOID_Data = GEOID,\n         median_income = B19013e1) \n\n\n##check types to varify that they are the same class \n#class(median_income_df$GEOID_Data) == class(census$GEOID_Data)\n\njoined_income_census &lt;- left_join(census, \n                         median_income_df, \n                         by = \"GEOID_Data\")\n\n\n## filter census data by buildings determined to be impacted by blackouts. \n\n# transforming both objects to the correct crs\njoined_income_census &lt;- st_transform(joined_income_census, crs = st_crs(census))\nbuildings_blackout &lt;- st_transform(buildings_blackout, crs = st_crs(census))\n\n#check that they are the same crs \n#st_crs(joined_income_census) == st_crs(buildings_blackout)\n\n## filtering and mutating to add a blackout column for affected tracts \ncensus_blackout &lt;- joined_income_census[buildings_blackout,]%&gt;% mutate(blackout = 'Y')\n\nprint(paste0(nrow(census_blackout), \" census tracts had blackouts\"))\n\n\n[1] \"778 census tracts had blackouts\"\n\n\nSince we have blackout status of Y indicated, we will want to indicate “N” for no-blackout areas as well, for full dataset.\n\n\nComparing incomes of impacted tracts to unimpacted tracts\nTo do this, we create a map using tmap of median income by census tract, designating which tracts had blackouts. We then plot the distribution of income in impacted and unimpacted tracts.\nI included the code for the map here, although the map does not render. I’ve attached a screeshot\n\n\nCode\n#join income data to the census tract geometries and crop to region of interest\ncensus_income_geom &lt;- left_join(census, median_income_df, \n                         by = \"GEOID_Data\") %&gt;% st_transform(3083) #this is texas#\nhouston_border_sf &lt;- houston_border_sf %&gt;% st_transform(3083)\n\n\n#spatial crop joined census data to houston border \ncensus_income_geom_cropped &lt;- census_income_geom[houston_border_sf, op = st_intersects] \n\n\n## use tmap to plot income distribution and affected areas. Make graph aesthetics. \n# tmap_mode(\"plot\")\n# tm_shape(census_income_geom_cropped) + \n#   tm_graticules() + \n#   tm_polygons(\"median_income\", palette= \"-viridis\", title = \"Median Income\") + \n#   tm_shape(census_blackout) + tm_dots(size = 0.4, alpha = 0.5) + \n#   tm_title( \"Blackout Status & Median Income by Census in Houston (Feb 16,2021)\") +\n#   tm_scalebar(position = c(\"RIGHT\", \"BOTTOM\")) + \n#   tm_compass(position = c(\"LEFT\", \"TOP\")) + \n#   tm_xlab(\"Longitude\", size = 0.5) + \n#   tm_ylab(\"Latitude\", rotation = 90, size = 0.5)"
  },
  {
    "objectID": "posts/2023-12-15-texas/index.html#discussion",
    "href": "posts/2023-12-15-texas/index.html#discussion",
    "title": "2021 Texas Blackout",
    "section": "Discussion",
    "text": "Discussion\nWe found that 778 census tracts had been affected by Texas’s 2021 energy crisis, amounting to 164,867 total homes experiencing power outage between the studied dates (Feb 07, 2021 to Feb 16, 2021).\nThe average median income for homes that experienced a blackout was $70,939, slightly higher than the average median income for homes that didn’t experience a blackout, which was $67,859. It is important to note that 1) our factor of interest was median income within census tracts and that other socioeconomic factors could reveal unexplored spatial patterns, and that 2) weather conditions were not normalized to conduct this exercise, variable that could cause a different outcome."
  },
  {
    "objectID": "posts/2023-12-15-texas/index.html#references",
    "href": "posts/2023-12-15-texas/index.html#references",
    "title": "2021 Texas Blackout",
    "section": "References",
    "text": "References\n[1] Ball, J. (Feb, 2021). The Texas Blackout is the Story of a Disaster Foretold. Texas Monthly. URL: https://www.texasmonthly.com/news-politics/texas-blackout-preventable/\n[2] Henson, Bob. (Feb, 2021). Why the power is out in Texas… and why other states are vulnerable too. Yale Climate Connections. URL: https://yaleclimateconnections.org/2021/02/why-the-power-is-out-in-texas-and-why-other-states-are-vulnerable-too/\n[3] Irfan, U. (Mar, 2021). Why every state is vulnerable to a Texas-style power crisis. Vox. URL: https://www.vox.com/22308149/texas-blackout-power-outage-winter-uri-grid-ercot\n[4] National Center for Disaster Preparedness (NCDP). (Mar, 2023). Disaster Response and Equity: Reflecting on the Racial Disparities in Texas Power Outages. URL: https://ncdp.columbia.edu/ncdp-perspectives/disaster-response-and-equity-texas-power-outrages/"
  },
  {
    "objectID": "posts/2024-06-18-sensitivity-maps/index.html#tools-used",
    "href": "posts/2024-06-18-sensitivity-maps/index.html#tools-used",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Tools Used",
    "text": "Tools Used"
  },
  {
    "objectID": "posts/2024-06-18-sensitivity-maps/index.html#tree-sensitivty-background",
    "href": "posts/2024-06-18-sensitivity-maps/index.html#tree-sensitivty-background",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Tree Sensitivty Background",
    "text": "Tree Sensitivty Background\nForests cover approximately 30% of Earth’s land surface, absorb more carbon than all other terrestrial ecosystems, and provide trillions of dollars’ worth of ecosystem services (Food and Agriculture Organization of the United Nations, 2005). However, forests are increasingly threatened by climate change-induced shifts in drought frequency and severity. As temperatures inceases, it is critical to develop effective management strategies and identify techniques to prioritize management interventions. For example, developing cutting-edge models that can identify regions within a species’ range that are more vulnerable to drought can bolster restoration efforts, particularly for threatened species and species exposed to frequent droughts."
  },
  {
    "objectID": "posts/2024-06-18-sensitivity-maps/index.html#mapping-tree-sensitivty-dashboard",
    "href": "posts/2024-06-18-sensitivity-maps/index.html#mapping-tree-sensitivty-dashboard",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Mapping Tree Sensitivty Dashboard",
    "text": "Mapping Tree Sensitivty Dashboard\nThe Mapping Tree Sensitivity Dashboard assses species-specific sensitivity to drought by quantifying variation in drought-sensitivity across tree species’ ranges. This localized information is citical for land managers to develop targeted drought-adaptation strategies. By analyzing variation in drought-sensitivity for 26 tree species, we demonstrate that th eimpacts of drier conditions vary by species and across species’ ranges. Our findings suggest that effective management strategies will need to consider species-level variation in drought sensitivity to sustain ecosystem services under climate change.\nWe hope this dashboard can act as a tool for land management partners to identify regions where management interventions may be needed under climate change"
  },
  {
    "objectID": "posts/2024-06-18-sensitivity-maps/index.html#data",
    "href": "posts/2024-06-18-sensitivity-maps/index.html#data",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Data",
    "text": "Data\nThis workflow builds off the existing framework developed by our client Dr. Joan Dudney, and her colleagues Dr. Robert Heilmayr, and Dr. Frances C Moore. Original code scripts and available at the following repository: GitHub Repository - Treeconomics\nRaw data was accessed from the following public sources:\n\nTree ring data - International Tree Ring Data Bank\n\n\nDate of Access: 2020-07-05\nVersion: ITRDB v.7.22\n\n\nClimate data - Terra Climate\n\n\nDate of Access: 2024-04-02\nVersion: Annual data (1958-present)"
  },
  {
    "objectID": "posts/2024-06-18-sensitivity-maps/index.html#reinterpreting-tree-sensitivty-scales",
    "href": "posts/2024-06-18-sensitivity-maps/index.html#reinterpreting-tree-sensitivty-scales",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Reinterpreting Tree Sensitivty Scales",
    "text": "Reinterpreting Tree Sensitivty Scales\nA live website of the dashboard is hosted by the Bren School of Environmental Management. Visit our live dashboard here\nWithin the dashboard, we used the R {leaflet} package to plot the interactive maps. The coordinate reference system is set to WGS84. The original sensitivity maps had a continuous scale of sensitivity ranging from negative to postive numbers. In order to increase interpretation of sensitivity maps, the categorical bins and orange-blue color palettes were create in order to signal sensitivty or non-senstivty. Therefore, any positive values of sensitivty mean these are species of little to no concern. The Negative scale was divided into quantiles of High, Moderate, and Low Sensitivty which takes in either three or four colors depending on the levels of sensitivity found in the dataset."
  },
  {
    "objectID": "posts/2024-06-18-sensitivity-maps/index.html#dashboard",
    "href": "posts/2024-06-18-sensitivity-maps/index.html#dashboard",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Dashboard",
    "text": "Dashboard\nThe Interactive Dashboard was built using R, R {Shiny} package, and {Leaflet} package. The intention of this dashboard is too choose a tree species of interest either by species code, common name or scientific name, and map the raster map of sensitivty values for each map. You can see how sensitivty ranges from tree to tree. Another feature I added was changing map tiles and transparency to better visualize the raster maps.\nBelow is a short video of our dashboard."
  },
  {
    "objectID": "posts/2023-12-15-water-stress/index.html",
    "href": "posts/2023-12-15-water-stress/index.html",
    "title": "Water Stressors for Low Income Food Deficit Countries",
    "section": "",
    "text": "Background and Motivation\nWater is the worlds most valuable resource and climate change will likely exacerbate water stress world wide. Water stress will further affect the most vulnerable countries. Vulnerable countries are already facing water shortages, poor water management, privatized water management, etc. Water stressors include demands from agriculture, natural disasters, industrial use , and municipal use. Water stress occurs when demand for safe, usable water in a given area exceeds the supply. I thought a good measure of vulnerability would be to study water stressors in Low income food deficit countries(LIFDCs) given that vulnerable countries often face issues of poverty and food scarcity that are often linked to water vulnerability.\nAs global population grows (increasing agricultural, industrial and domestic demands for water), and water demand increases, water stress and the risk of water scarcity is now a common concern. This is even more applicable for particular regions with lower water resources and/or larger population pressures.\nIn this study I chose to focus on the most vulnerable areas for certain reasons\na. challenge myself with missing data\nb. stray away from Eurocentric studies that focus on Western development\nc. Highlight countries of need\n\n\n\nGoal\nMy overall question is: What are the biggest stressors of water resources for “low income food deficit countries?” Understanding what the water stressors are in each country\n\n\n\nLow Income Food Deficit Countries\n\n\n\n\nAQUASTAT Data\n\nAbout the Data\nThe AQUASTAT Dataset is the Food and Agriculure Orgnaization of the United Nations global information system on water resources and agriculture water management. It collects, analyses and provides free access to over 180 variables and indicators by country from 1960. AQUASTAT draws on national capacities and expertise with an emphasis on Africa, the Near East, countries of the former Soviet Union, Asia, and Latin America and the Caribbean. AQUASTAT plays a key role in the monitoring of the Sustainable Development Goal 6 that sets out to “ensure availability and sustainable management of water and sanitation for all”, and in particular indicators of target 6.4 on water stress and water use efficiency.\nFAO’s global Information System on Water and Agriculture can a new online platform that is The visit the AQUASTAT platform, you can visit: AQUASTAT.\n\n\n\nAnalysis Plan\n\nClean Data for Variables of Interest\nExploratory Data Analysis\nVisualize Urban population of each country by Continent 3 most important water withdrawals\n\nIrrigation\nIndustrial\nMunicipal\n\nLinear Model Relationship between Water Stress and Urban Population Relationship between Cultivated area (arable land + permanent crops)\nTime Series Data Annual freshwater withdrawals, 2019\n\nData Cleaning\nAlthought the AQUASTAT dataset was very detailed, cleaning the data resulted in a challenge. Downloading the dataset from the online platform made, resulted in a dataset with a Country, Variable, and Unit column. Below is the data cleaning process. Essentially, I had to pivot the entire table and rejoin units with their variable.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(here)\nlibrary(readr)\nlibrary(janitor)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(countrycode)\nlibrary(knitr)\nlibrary(sf)\nlibrary(wbstats)\nlibrary(rnaturalearth)\n\nlibrary(raster)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tidyverse)\nlibrary(ggspatial)\nlibrary(patchwork)\n\nlibrary(tmap)\n\n\n\n\nCode\nwater_resources &lt;- read_csv(\"data/AQUASTAT_Water_Resources.csv\")\n\n# water_data &lt;- water_resources %&gt;% \n#   subset(\"Unit\", \"Symbol\") + \n#   group_by(Country, Variable) %&gt;%\n#   pivot_wider(names_from = Variable,\n#               values_from = c('\"2020\"', Symbol, Unit))\n# \n# water_data\n\nwater &lt;- read_csv(\"data/AQUASTAT_Dissemination_System.csv\")\ncolnames(water)\n\n#water &lt;-  water %&gt;% select(-\"...6\")\n\nwater5 &lt;- water \n\nsubwater &lt;- subset(water, select = -c(Variable, Unit)) \n\ncombin &lt;- paste(water$Variable, water$Unit, sep = \"_\")\n\ncombin &lt;- data.frame(combin)\n\nwater = cbind(subwater, combin)\n\ncolnames(water) &lt;- c(\"Country\", \"Symbol\", \"2020\", \"Variable\")\n\nwater1 &lt;- water %&gt;% \n  subset(select = -Symbol) %&gt;% \n  group_by(Country) %&gt;%\n  pivot_wider(names_from = (Variable),\n              values_from = \"2020\") \n\ncolnames(water1)\n\n# ---Add a new column \"Continent\" to water1 that corresponds to Country---\nwater1 &lt;- water1 %&gt;%\n  mutate(Continent = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"continent\")) %&gt;% \n  dplyr::rename(Urban_population = `Urban population_1000 inhab`)\n\nwater1\n\n\n\n\nExploratory Data Analysis\nI found that the countries that are labeld LIFDCs belong to 3 continents: Asia, Africa, and the Americas with the majority in the African Continent. Below are the countries and the relative Urban Population to each country by continent.\n\nMap of Urban Population of Asia\n\nMap of Urban Population of AfricaMap of Urban Population of AsiaMap of Urban Population of Americas\n\n\n\n\nCode\nafrica = world |&gt; \n  filter(continent == \"Africa\", !is.na(iso_a2)) %&gt;% \n  dplyr::rename(Country = name_long) %&gt;%\n  right_join(urban_pop_africa, by = \"Country\") %&gt;% \n  # dplyr::select(name, subregion, gdpPercap, HDI, pop_growth, urban_pop) %&gt;% \n  st_transform(\"+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25\")\n\n\nafrica_map&lt;- ggplot(africa) +\n  geom_sf() +\n  theme_bw()\n\nurban_map_africa &lt;- ggplot(africa) +\n  # geom_sf(aes(color) = HDI)) +\n  geom_sf(aes(fill = Urban_population)) +\n  theme_bw() \n\nafrica_map+urban_map_africa\n\n\n\n\n\nAfrica Urban Population\n\n\n\n\n\n\nCode\nasia = world |&gt; \n  filter(continent == \"Asia\", !is.na(iso_a2)) %&gt;% \n  dplyr::rename(Country = name_long) %&gt;%\n  right_join(urban_pop_asia, by = \"Country\") %&gt;% \n  # dplyr::select(name, subregion, gdpPercap, HDI, pop_growth, urban_pop) %&gt;% \n  st_transform(\"+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25\")\n\n\nasia_map&lt;- ggplot(asia) +\n  geom_sf() +\n  theme_bw()\n \nurban_map_asia &lt;- ggplot(asia) +\n  # geom_sf(aes(color) = HDI)) +\n  geom_sf(aes(fill = Urban_population)) +\n  theme_bw() \n\nasia_map+urban_map_asia\n\n\n\n\n\nAsia Urban Population\n\n\n\n\n\n\nCode\namerica = world |&gt; \n  filter(continent == \"America\", !is.na(iso_a2)) %&gt;% \n  dplyr::rename(Country = name_long) %&gt;%\n  right_join(urban_pop_america, by = \"Country\") %&gt;% \n  # dplyr::select(name, subregion, gdpPercap, HDI, pop_growth, urban_pop) %&gt;% \n  st_transform(\"+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25\")\n\n\namerica_map&lt;- ggplot(america) +\n  geom_sf() +\n  theme_bw()\namerica_map  \nurban_map_america &lt;- ggplot(america) +\n  # geom_sf(aes(color) = HDI)) +\n  geom_sf(aes(fill = Urban_population)) +\n  theme_bw() \n\namerica_map+urban_map_america\n\n\n\n\n\nAmericas Urban Population\n\n\n\n\n\n\n\nAgricultural Water Withdrawls and GDP\nWater stress is defined based on the ratio of freshwater withdrawals to renewable freshwater resources. Water stress does not insinuate that a country has water shortages, but does give an indication of how close it maybe be to exceeding a water basin’s renewable resources. If water withdrawals exceed available resources (i.e. greater than 100 percent) then a country is either extracting beyond the rate at which aquifers can be replenished, or has very high levels of desalinisation water generation (the conversion of seawater to freshwater using osmosis processes).\nBefore Running the linear regression model, I plotted the agricultural water withdrawal versus GDP per capital. I figured that GDP is could be an indicator of urbanization.\n\n\nCode\nggplot(data = urban_pop_asia, aes(x = GDP, y = `Agricultural Water Withdrawal`)) +\n  geom_point() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Adjust the angle as needed\n  geom_text(aes(label = Country), vjust = -0.5, hjust = 1) +  # Adjust vjust and hjust as needed\n  labs(title = \"Agricultural water withdrawals vs. GDP per capita, 2019\",\n       x = \"GDP per Capita\",\n       y = \"Annual freshwaer withdrawals, agricuture(% of total freshwater withdrawl)\") +\n  theme_minimal()\n\n\n\n\n\nAgricultural water withdrawals vs. GDP per capita, 2019\n\n\nThis premilniary plot shows that opposite of my prediction–that increased GDP would increase agricultural water withdrawal. Overall, we see a negative correlation: agriculture’s share of total water withdrawals tend to decrease at higher incomes.\nThe Asian LIFD countries in the plot(clustered around the upper left of the plot) used more water for agriculture.\n\n\n\nAnalysis - Linear Regression\n\nRelationship Between Urban Population and Water Stress in Asia\nFor this analysis I ran a linear regression to see if there was some relationship between urban population and water stress. From previous knowledge, we can expect the highter the urban population, the more water stress we can expect. An increase in urban populations might exasperate water withrawals in cities. Droughts may be exasperated in more urban areas due to increase temperatures from urbanization, transportions,\n\n\nCode\nwater_select &lt;- water1 %&gt;% \n  select(c(\"SDG 6.4.2. Water Stress_%\",\"Urban population_1000 inhab\", \"Country\")) %&gt;% \n  mutate(`Urban population_1000 inhab` = as.numeric(`Urban population_1000 inhab`)) %&gt;% \n  mutate(`SDG 6.4.2. Water Stress_%` = as.numeric(`SDG 6.4.2. Water Stress_%`))\n\nwater_select\nwater_stress_lm &lt;- lm(` Water Stress_%` ~ `Urban population_1000 inhab`, data = water_select)\nsummary(water_stress_lm)\n\n\nat\nThe linear model used here does not conclude to a definite conclusion. The R-squared measurement of 2.7%. This was to be expected. From the EDA step, we saw a negative correlation between Agricultural water withdrawal and GDP. GDP is a fair indicator of urbanization therefore a low r-squared value seems to be on the right track.\n\n\n\nAnalysis - Time Series of Freshwater Withdrawls (Asia)\nHere, I atttempted to use a Time Series of the Freshwater and Municipial water withdrawals.\nIrrigation water withdrawal normally far exceeds the net irrigation water requirement because of water lost in its distribution from its source to the crops.\nAssessing the impact of irrigation on water resources requires an estimate of the water effectively withdrawn for irrigation, i.e. the volume of water extracted from rivers, lakes and aquifers for irrigation purposes\n\n\nCode\nannual_freshwawter_withdrawls$Year &lt;- as.Date(AirPassengers$Year)\n\n# Create a time series plot\nts_annual_freshwater &lt;- annual_freshwawter_withdrawls %&gt;% \n  group_by(Year) %&gt;% \n  ggplot( aes(x = Year, y = `Annual freshwater withdrawals, total (billion cubic meters)`, color = Entity)) +\n  geom_point() +\n  labs(title = \"Annual freshwater withdrawals, 1975 to 2019\",\n       x = \"Year\",\n       y = \"total water withdrawalin cubic metres (m³) per year\")\nts_annual_freshwater \n\n\n\n\n\nFresh Water Withdrawal 1975 - 2019, Asia\n\n\nInterpretation I chose Asian countries becuase it was the easiest to visualize in one plot. We can see that there is missing data for many countries up until 1995, therefore it makes for a\n\n\nAnalysis - Time Series of Municipial Water Withdrawals (Asia)\nMunicipal water withdrawal\nTotal water withdrawal for municipal (domestic) purposes, measured in cubic metres (m³) per year. Municipal water is the annual quantity of water withdrawn primarily for the direct use by the population.\n\n\nCode\nannual_minicipial_withdrawls$Year &lt;- as.Date(AirPassengers$Year)\n\n# Create a time series plot\nts_annual_municipial &lt;- annual_minicipial_withdrawls %&gt;% \n  group_by(Year) %&gt;% \n  ggplot( aes(x = Year, y = `Annual municipial withdrawals, total (billion cubic meters)`, color = Entity)) +\n  geom_point() +\n  labs(title = \"Annual minicipial withdrawals, 1975 to 2019\",\n       x = \"Year\",\n       y = \"total water withdrawalin cubic metres (m³) per year\")\nts_annual_freshwater \n\n\n\n\n\nMunicipial Water Withdrawl from 1970 to 2015,Asia\n\n\nInterpretation I chose Asian countries becuase it was the easiest to visualize in one plot. We can see that there is missing data for many countries up until 1995. I Also omitted India, because if it outlying measurements.\n\n\nSummary\nWater scarcity is a growing concern in vulnerable countries that have predisposed risks due to climate change. Understanding the causes of these stressors–whether it be industrial, municipal or agricultural–is a step in the right direction to mitigate water scarcity. This project worked as a preliminary analysis to understand which stressor is most affecting LIFDCs. I focused on urbanization due to the fact that urban centers are a key part of a country’s development. As LIFDCs work their way out of this label, urbanization will be key part in a populations climb from rural to urban. From the linear regression model, we can concluded that the relationship between urban population and water stress is inconclusive. There are other factors that affect water stress.\n\n\nLimitations and Next Steps\nThere is a lot of missing data for LIFDCs due to privatized water management systems and low resources. These countries also face violations to water rights therefore data is scarce and not published due to fear. The next steps would be a further investigation to missing data. A key part of water knowledge would be community based knowledge, although this analysis project may need to be scaled down.\n\n\nReferences\nHannah Ritchie and Max Roser (2018)-“Water Use and Stress” Published online at OurWorldInData.org. Retrieved from: ‘https://ourworldindata.org/water-use-stress’ [Online Resource]\n(2021)-“AQUASTAT - FAO’s Global Information System on Water and Agriculture” Published online at www.fao.org. Retrieved from: ‘https://www.fao.org/aquastat/en/data-analysis/irrig-water-use/’ [Online Resource]\n(2021)-“AQUASTAT Dissemination System” Published online at www.fao.org. Retrieved from: ‘https://data.apps.fao.org/aquastat/?lang=en’ [Online Resource]\n\n\n\n\nCitationBibTeX citation:@online{salgado2023,\n  author = {Salgado, Vanessa},\n  title = {Water {Stressors} for {Low} {Income} {Food} {Deficit}\n    {Countries}},\n  date = {2023-12-15},\n  url = {https://github.com/Vanessa-Salgado},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSalgado, Vanessa. 2023. “Water Stressors for Low Income Food\nDeficit Countries.” December 15, 2023. https://github.com/Vanessa-Salgado."
  },
  {
    "objectID": "posts/2024-03-14-carbon-data-viz/index.html",
    "href": "posts/2024-03-14-carbon-data-viz/index.html",
    "title": "Carbon Credits Data Visualization",
    "section": "",
    "text": "The Paris Agreement of 2015 was an international standard laid out in order to reduce \\(CO_2\\) emissions. With the Paris Aggreement, national emmisions target and regulations sprung up in order to substantially reduce global greenhouse gas emmission to hold global temperature incear to well below 2 C.\nWith these new regulations, businesses were pressured to lower their emmissions. Thuse introducing carbon markets with the promise to turn \\(CO_2\\) into a commodity.\nCarbon offsetting is a trading mechanism that allows entities such as governments, individuals, or businesses to compensate for their greenhouse gas emissions by supporting projects that reduce, avoid, or remove emissions elsewhere.\nTo Put it simply, carbon offsets involve an entity that emits greenhouse gases into the atmosphere paying for another entity to pollute less. For example, an airline in a developed country that wants to claim it is reducing its emissions can pay for a patch of rainforest to be protected in the Amazon.\nThe main research question I want to investigate is whether carbon credits are an effective way to measure carbon offset emissions. Each credit purportedly offsets a metric tonne of CO2 emissions, yet the Berkeley Carbon Trading Project reports that, “research performed by [them] and others has found that many, if not most, offset credits traded on the market today do not represent real emissions reductions.” [[^1]]"
  },
  {
    "objectID": "posts/2024-03-14-carbon-data-viz/index.html#background-and-motivation",
    "href": "posts/2024-03-14-carbon-data-viz/index.html#background-and-motivation",
    "title": "Carbon Credits Data Visualization",
    "section": "",
    "text": "The Paris Agreement of 2015 was an international standard laid out in order to reduce \\(CO_2\\) emissions. With the Paris Aggreement, national emmisions target and regulations sprung up in order to substantially reduce global greenhouse gas emmission to hold global temperature incear to well below 2 C.\nWith these new regulations, businesses were pressured to lower their emmissions. Thuse introducing carbon markets with the promise to turn \\(CO_2\\) into a commodity.\nCarbon offsetting is a trading mechanism that allows entities such as governments, individuals, or businesses to compensate for their greenhouse gas emissions by supporting projects that reduce, avoid, or remove emissions elsewhere.\nTo Put it simply, carbon offsets involve an entity that emits greenhouse gases into the atmosphere paying for another entity to pollute less. For example, an airline in a developed country that wants to claim it is reducing its emissions can pay for a patch of rainforest to be protected in the Amazon.\nThe main research question I want to investigate is whether carbon credits are an effective way to measure carbon offset emissions. Each credit purportedly offsets a metric tonne of CO2 emissions, yet the Berkeley Carbon Trading Project reports that, “research performed by [them] and others has found that many, if not most, offset credits traded on the market today do not represent real emissions reductions.” [[^1]]"
  },
  {
    "objectID": "posts/2024-03-14-carbon-data-viz/index.html#goal",
    "href": "posts/2024-03-14-carbon-data-viz/index.html#goal",
    "title": "Carbon Credits Data Visualization",
    "section": "Goal",
    "text": "Goal\nCombining the Voluntary Registry Offsets Data with another emissions data would be a helpful way to measure if carbon trading/offsets are accurate in measuring emissions.\n\nReserach Questions\n\nThe main research question is to see : Are climate credits being distrubuted evenly?\n\nI will create three separate visualizations that same overall question: How has the Carbon Market grown over the years per country ? and are they keeping up with the global emission reduction goals\nEach visualization below has been"
  },
  {
    "objectID": "posts/2024-03-14-carbon-data-viz/index.html#data",
    "href": "posts/2024-03-14-carbon-data-viz/index.html#data",
    "title": "Carbon Credits Data Visualization",
    "section": "Data",
    "text": "Data\n\nLink to (or otherwise prove the existence of) at least one data set that you plan to use for Assignment #4\n\nBerkeley Voluntary Registry Offsets Database: contains all carbon offset projects, credit issuances, and credit retirements listed globally by four major voluntary offset project registries\nData on CO2 and Greenhouse emissions by Our World in Data\nRaster Data provided by Natural Earth\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                           library setup                                 ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(ggExtra)\nlibrary(gghighlight)\nlibrary(janitor)\nlibrary(naniar)\nlibrary(RColorBrewer)\nlibrary(here)\nlibrary(forcats)\n\nlibrary(showtext)\nfont_add_google(name = \"Merriweather\", family = \"merriweather\")\nfont_add_google(name = \"Karla\", family = \"karla\")\n\nfont_add(family = \"fa-6-brands\",\n         regular = here::here(\"fonts\", \"Font Awesome 6 Brands-Regular-400.otf\"))\nfont_add(family = \"fa-6-regular\",\n         regular = here::here(\"fonts\", \"Font Awesome 6 Free-Regular-400.otf\")) \nfont_add(family = \"fa-6-solid\",\n         regular = here::here(\"fonts\", \"Font Awesome 6 Free-Solid-900.otf\"))\n\nfont_add(family = \"fa-brands\",\n         regular = here::here(\"fonts\", \"fa-brands-400.ttf\"))\nfont_add(family = \"fa-regular\",\n         regular = here::here(\"fonts\", \"fa-regular-400.ttf\")) \nfont_add(family = \"fa-solid\",\n         regular = here::here(\"fonts\", \"fa-solid-900.ttf\"))\n\n\nlibrary(raster)\nlibrary(countrycode)\n\nlibrary(ggtext)\nlibrary(ggrepel)\nlibrary(patchwork)\nlibrary(systemfonts)\n\nlibrary(fmsb)\nlibrary(stringr)\n\nlibrary(emojifont)\nlibrary(NatParksPalettes)\n\nlibrary(waffle)"
  },
  {
    "objectID": "posts/2024-03-14-carbon-data-viz/index.html#data-wrangling",
    "href": "posts/2024-03-14-carbon-data-viz/index.html#data-wrangling",
    "title": "Carbon Credits Data Visualization",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                          data cleaning & wrangling                       ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ncarbon_offsets_clean &lt;- carbon_offsets %&gt;%\n  janitor::clean_names(case = \"snake\") %&gt;%\n  replace_with_na_all(condition = ~.x %in% c(-99999, \"#REF!\", \"N/a\", \"N/A\", \"NA\")) %&gt;%\n\n  # change char types to ints or doubles\n  mutate(total_credits_issued = as.numeric(gsub(\",\", \"\", total_credits_issued))) %&gt;%\n  mutate(total_credits_retired = as.numeric(gsub(\",\", \"\", total_credits_retired))) %&gt;%\n  mutate(total_credits_remaining = as.numeric(gsub(\",\", \"\", total_credits_remaining))) %&gt;%\n  mutate(total_buffer_pool_deposits = as.numeric(gsub(\",\", \"\", total_buffer_pool_deposits))) %&gt;%\n  mutate(first_year_of_project = as.factor(gsub(\",\", \"\", first_year_of_project))) %&gt;%\n\n  # remove irregular year columns of the structure 2001...127\n  #select_if(~!any(grepl(\"\\\\d\", .)))\n  dplyr::select(!matches(\"\\\\d\"))\n  # select(-grep(pattern = \"^[0-9]{4}\\\\.\\\\.\\\\.[0-9]{3}$\", names(.), value = TRUE)) %&gt;%\n\nyear_of_offsets &lt;- year_of_offsets %&gt;%\n  rename(year_of_offset = year)\n\ncarbon_offsets_clean_df &lt;- carbon_offsets_clean %&gt;%\n  # join with regular year data\n  full_join(year_of_offsets, by = 'project_name',relationship = \"many-to-many\")\n\n\nAfter data cleaning, I saved the cleaned dataset so that I would not have to run the cleaning script everytime. Here I simply read in the cleaned version of this carbon_offsetsdataset.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                          importing data                      ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ncarbon_offsets_clean &lt;- read_csv(here(\"data\",\"carbon_offsets_clean.csv\"))\n\n# color palettes\n\nglacier = c(\"#01353D\", \"#088096\", \"#58B3C7\", \"#7AD4E4\", \"#B8FCFC\")\neverglades = c(\"#345023\", \"#596C0B\", \"#83A102\", \"#003B68\", \"#426F86\", \"#7A712F\")\nigauzufalls = c(\"#415521\", \"#97AD3D\", \"#4C3425\", \"#7F6552\", \"#5A8093\", \"#9FBAD3\")\nolympic = c(\"#3A4330\", \"#426737\", \"#75871B\", \"#BAB97D\", \"#FDE16A\", \"#F9B40E\", \"#E88C23\", \"#A25933\")"
  },
  {
    "objectID": "posts/2024-03-14-carbon-data-viz/index.html#infographic",
    "href": "posts/2024-03-14-carbon-data-viz/index.html#infographic",
    "title": "Carbon Credits Data Visualization",
    "section": "Infographic",
    "text": "Infographic\n\nPlease look at a better Resolution Photo on my Github Repo : How are Carbon Credits Distributed Infographic"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am currently on the job search for Data Analytics or Database management in an environmental or social sphere in the larger Los Angeles metro area. Aside from job searching, I have started my own business selling vintage cameras and watches. I have more details on that below.\nThis website is a work in progress, and I hope to update with each milestone or progress on my projects."
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About Me",
    "section": "",
    "text": "I enjoy a concoction of hobbies at any given time but I am persistent on learning french, finding the next hidden gem at the thrift store and trying out new sports.\nThings I enjoy to slow things down are reading and trying out new recipes. Things I learned a long they way that I am not the greatest baker."
  },
  {
    "objectID": "about.html#eductional-background",
    "href": "about.html#eductional-background",
    "title": "About Me",
    "section": "Eductional Background",
    "text": "Eductional Background\nMaster of Environmental Data Science - University of California, Santa Barbara (June 2024)\nB.S Computer Science 2022 - University of California, Santa Barbara"
  },
  {
    "objectID": "about.html#current-fascinations",
    "href": "about.html#current-fascinations",
    "title": "About Me",
    "section": "Current Fascinations",
    "text": "Current Fascinations\n\nCamera BusinessVintage Watch BusinessProductivity App\n\n\nMy partner and I started this vintage camera business as a subdivision of his vintage reselling business. We resell 2000s cameras and film cameras to those who also want to capture memories through a vintage lens. You can find us on instagram @MomentsOnDigi or at local LA flea markets.\n\n\nI love going to flea markets and I always seem to find the prettiest, daintiest watches. I would always get compliments on watches and wanted to share my love for watches at flea markets as well or on Etsy!"
  },
  {
    "objectID": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#tools-used",
    "href": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#tools-used",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Tools Used",
    "text": "Tools Used"
  },
  {
    "objectID": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#tree-sensitivty-background",
    "href": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#tree-sensitivty-background",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Tree Sensitivty Background",
    "text": "Tree Sensitivty Background\nForests cover approximately 30% of Earth’s land surface, absorb more carbon than all other terrestrial ecosystems, and provide trillions of dollars’ worth of ecosystem services (Food and Agriculture Organization of the United Nations, 2005). However, forests are increasingly threatened by climate change-induced shifts in drought frequency and severity. As temperatures inceases, it is critical to develop effective management strategies and identify techniques to prioritize management interventions. For example, developing cutting-edge models that can identify regions within a species’ range that are more vulnerable to drought can bolster restoration efforts, particularly for threatened species and species exposed to frequent droughts."
  },
  {
    "objectID": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#mapping-tree-sensitivty-dashboard",
    "href": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#mapping-tree-sensitivty-dashboard",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Mapping Tree Sensitivty Dashboard",
    "text": "Mapping Tree Sensitivty Dashboard\nThe Mapping Tree Sensitivity Dashboard assses species-specific sensitivity to drought by quantifying variation in drought-sensitivity across tree species’ ranges. This localized information is citical for land managers to develop targeted drought-adaptation strategies. By analyzing variation in drought-sensitivity for 26 tree species, we demonstrate that th eimpacts of drier conditions vary by species and across species’ ranges. Our findings suggest that effective management strategies will need to consider species-level variation in drought sensitivity to sustain ecosystem services under climate change.\nWe hope this dashboard can act as a tool for land management partners to identify regions where management interventions may be needed under climate change"
  },
  {
    "objectID": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#data",
    "href": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#data",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Data",
    "text": "Data\nThis workflow builds off the existing framework developed by our client Dr. Joan Dudney, and her colleagues Dr. Robert Heilmayr, and Dr. Frances C Moore. Original code scripts and available at the following repository: GitHub Repository - Treeconomics (opens in new window) Raw data derivation: Raw data was accessed from the following public sources: 1. Tree ring data - International Tree Ring Data Bank (opens in new window) Date of Access: 2020-07-05 Version: ITRDB v.7.22 2. Climate data - Terra Climate (opens in new window) Date of Access: 2024-04-02 Version: Annual data (1958-present)"
  },
  {
    "objectID": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#reinterpreting-tree-sensitivty-scales",
    "href": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#reinterpreting-tree-sensitivty-scales",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Reinterpreting Tree Sensitivty Scales",
    "text": "Reinterpreting Tree Sensitivty Scales\nA live website of the dashboard is hosted by the Bren School of Environmental Management. It is currently live here:\nWithin the dashboard, we used the R {leaflet} package to plot the interactive maps. The coordinate reference system is set to WGS84. The original sensitivity maps had a continuous scale of sensitivity ranging from negative to postive numbers. In order to increase interpretation of sensitivity maps, the categorical bins and orange-blue color palettes were create in order to signal sensitivty or non-senstivty. Therefore, any positive values of sensitivty mean these are species of little to no concern. The Negative scale was divided into quantiles of High, Moderate, and Low Sensitivty which takes in either three or four colors depending on the levels of sensitivity found in the dataset."
  },
  {
    "objectID": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#dashboard",
    "href": "posts/CopyOf2024-06-18-sensitivity-maps-copy/index.html#dashboard",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Dashboard",
    "text": "Dashboard\nThe Interactive Dashboard"
  },
  {
    "objectID": "posts/2025-01-05-prez-analysis/index.html",
    "href": "posts/2025-01-05-prez-analysis/index.html",
    "title": "Presidential Sentiment Analysis Project",
    "section": "",
    "text": "This project was created by both me and Alan Feria. Using data analysis and machine learning algorithms, we hope to explore the sentiment and opinions of presidential candidates with a data lens. Our github can be found here.\n\nProject Summary\nOur project aims to use text analysis to examine the sentiment surrounding the 2024 Presidential and Vice-Presidential candidates found in news outlets and social media leading up to the presidential election on November 2nd. We will do this by using sentiment analysis and examining the reliability of online text information. We will also create an AI bot that will use the online news and social media text to generate its own opinion based on web-scraped information. Our overall question is: How do different sources of media impact a voter’s view & opinion on the 2024 presidential election?\n\n\nProblem Description\nAs the 2024 election approaches, misinformation and biased reporting on platforms like X, Instagram, and traditional outlets such as CNN, ABC, and Fox. This spread of information complicates public understanding. Our project aims to create user-friendly tools that evaluate the reliability of these sources while creating an AI bot that generates its own opinion based on these informational sources. We’ll explore how AI can generate informed perspectives on the election’s developments and how different news sources may impact a voter’s perspective on the election.\n\n\nProject Goals\n\nDevelop a robust full stack system that inputs, stores, and visualizes voter opinion across various sources, including new articles and social media posts.\nDesign an algorithm that analyzes sites based on multiple factors and assigns a credibility score to each source.\nDevelop a modular AI bot that generates its own opinion based on scraped data across different platforms.\nDevelop an interactive dashboard that enables users to explore and visualize sentiment and opinion data in a user-friendly interface.\n\n\n\nTimeline\nWe created this timeline to keep our team on track with the project goals.\n\nData Collection & Preprocessing\n\nIdentify and web scrape relevant text data from various news outlets and social media platforms (e.g., X, Instagram, CNN, Fox).\nClean and preprocess the data, including tokenization, noise removal , and other text normalization techniques.\nDevelop a data pipeline to ensure continuous data collection leading up to the election.\n\n\n\nSentiment Analysis Implementation\n\nImplement sentiment analysis algorithms to assess the sentiment in the collected texts, using tools like VADER, TextBlob, or machine learning-based models.\nCreate visualizations to represent the sentiment trends over time for each candidate and media source.\nBegin preliminary analysis to identify patterns in sentiment across different media sources.\n\n\n\nAI Bot Development\n\nDevelop and train the AI bot to generate opinions based on the collected and analyzed data.\nTest the bot’s opinion generation capabilities, refining the model based on a ccuracy and relevance.\n\n\n\nReliability Assessment & Media Impact Analysis\n\nDevelop a framework to evaluate the reliability of different media sources, possibly using metrics such as source credibility, historical accuracy, and bias detection.\nAnalyze the relationship between media source reliability and voter sentiment, exploring how misinformation or bias might skew public opinion\n\n\n\nDashboard Development\n\nIntegrate all components into a cohesive tool or platform, ensuring seamless interaction between the sentiment analysis, AI bot, and reliability assessments.\n\n\n\n\nCurrent Progress\nData Collection and Preprocessing\nUsing Python and Selenium, tweets from both Donald Trump and Kamala Harris and popular hashtags such as #PresidentialDebate2024, #MAGA2024, and #KamalaHarris2024 were scraped. About 10,000+ enteries were obtained during critical election periods such as presidential debates, government news, and/or policy changes.\nMy main contribution was creating a X, formerly Twitter, scraper to obtian tweets from both candidates and hashtags metioned above. Web scraping data needed to be done through the course of a week to not let X discover my automate Selenium bot. Each data entry had critical information such as author, time, content, hashtags.\nCleaning the data was a critical step, entries were duplicated becuase of tweet replies. In order to prepare data for sentiment analysis, the text of the tweets needed to be void of repetitive, non-meaningful words such as an, the, a, it, etc. and punctuation. The R packages {tokenizers}, {tidyverse} and {tidytext} were used to tokenize the body of tweets.\nProgress is pending…"
  },
  {
    "objectID": "posts/2024-06-18-sensitivity-maps/index.html#tree-sensitivity-background",
    "href": "posts/2024-06-18-sensitivity-maps/index.html#tree-sensitivity-background",
    "title": "Master’s Capstone Project: Mapping Tress Species Drought Sensitivity Under Climate Change",
    "section": "Tree Sensitivity Background",
    "text": "Tree Sensitivity Background\nForests cover approximately 30% of Earth’s land surface, absorb more carbon than all other terrestrial ecosystems, and provide trillions of dollars’ worth of ecosystem services (Food and Agriculture Organization of the United Nations, 2005). However, forests are increasingly threatened by climate change-induced shifts in drought frequency and severity. As temperatures inceases, it is critical to develop effective management strategies and identify techniques to prioritize management interventions. For example, developing cutting-edge models that can identify regions within a species’ range that are more vulnerable to drought can bolster restoration efforts, particularly for threatened species and species exposed to frequent droughts."
  },
  {
    "objectID": "about.html#currently",
    "href": "about.html#currently",
    "title": "About Me",
    "section": "",
    "text": "I am currently on the job search for Data Analytics or Database management in an environmental or social sphere in the larger Los Angeles metro area. Aside from job searching, I have started my own business selling vintage cameras and watches. I have more details on that below.\nThis website is a work in progress, and I hope to update with each milestone or progress on my projects."
  }
]